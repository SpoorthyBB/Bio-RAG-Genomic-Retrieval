{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CliF8pCeWyd"
   },
   "source": [
    "**Bio-RAG Research Assistant**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkShUQOIUwXr"
   },
   "source": [
    "**1. Setup and Installation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qDJbxXkVYuwy"
   },
   "outputs": [],
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ayMqx-NPZIBb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\spoor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\spoor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\spoor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\spoor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\spoor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL TEXT:\n",
      "\n",
      "CRISPR-Cas9 is a unique technology that enables geneticists and medical researchers\n",
      "to edit parts of the genome by removing, adding or altering sections of the DNA sequence.\n",
      "\n",
      "\n",
      "--- Processing ---\n",
      "\n",
      "FINAL TOKENS (Input for the Model):\n",
      "['crispr-cas9', 'unique', 'technology', 'enables', 'geneticist', 'medical', 'researcher', 'edit', 'part', 'genome', 'removing', 'adding', 'altering', 'section', 'dna', 'sequence']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# --- Step 0: Download NLTK resources (Run this once) ---\n",
    "# 'punkt' is for splitting sentences/words\n",
    "# 'stopwords' is the list of common useless words\n",
    "# 'wordnet' is the dictionary for lemmatization\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab') # sometimes needed for newer nltk versions\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Takes raw biological text and cleans it for the AI model.\n",
    "    Steps: Lowercase -> Tokenize -> Remove Punctuation/Stopwords -> Lemmatize\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Lowercasing\n",
    "    # Why: \"Gene\" and \"gene\" should be treated as the same thing by the model.\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Tokenization\n",
    "    # Why: Models can't read sentences; they read individual units (tokens).\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # 3. Setting up removals\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Customizing stopwords: In bio, we might want to keep words like 'not' (negation),\n",
    "    # but for now, we use the standard list.\n",
    "\n",
    "    # 4. Lemmatization Initialization\n",
    "    # Why: Lemmatization is smarter than Stemming.\n",
    "    # It converts \"studies\" -> \"study\" (valid word), whereas Stemming might do \"studie\" (garbage).\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "        # Remove punctuation and stop words\n",
    "        if token not in string.punctuation and token not in stop_words:\n",
    "            # Apply Lemmatization\n",
    "            lemma = lemmatizer.lemmatize(token)\n",
    "            clean_tokens.append(lemma)\n",
    "\n",
    "    return clean_tokens\n",
    "\n",
    "# --- Testing with a Biology Example (Relevant to Genomiki) ---\n",
    "raw_data = \"\"\"\n",
    "CRISPR-Cas9 is a unique technology that enables geneticists and medical researchers\n",
    "to edit parts of the genome by removing, adding or altering sections of the DNA sequence.\n",
    "\"\"\"\n",
    "\n",
    "processed_data = preprocess_text(raw_data)\n",
    "\n",
    "print(\"ORIGINAL TEXT:\")\n",
    "print(raw_data)\n",
    "print(\"\\n--- Processing ---\\n\")\n",
    "print(\"FINAL TOKENS (Input for the Model):\")\n",
    "print(processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jktrjy8Cj4Qu"
   },
   "source": [
    "**2. Data Ingestion & Vector Embedding Retrieval (Semantic Search)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xvDYgFUkNjnE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model... (Downloading from Hugging Face)\n",
      "\n",
      "--- SUCCESSFUL RESULT ---\n",
      "Query: How do we edit genomes?\n",
      "Match: CRISPR-Cas9 is a gene-editing tool.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Install the library\n",
    "#!pip install sentence-transformers\n",
    "\n",
    "# 2. Import libraries\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# 3. Load the Model\n",
    "print(\"Loading Model... (Downloading from Hugging Face)\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 4. Prepare Data\n",
    "sentences = [\n",
    "    \"CRISPR-Cas9 is a gene-editing tool.\",\n",
    "    \"DNA consists of two strands.\",\n",
    "    \"The mitochondria is the powerhouse of the cell.\",\n",
    "    \"Python is a programming language.\"\n",
    "]\n",
    "\n",
    "# 5. Create Embeddings\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# 6. Search\n",
    "query = \"How do we edit genomes?\"\n",
    "query_embedding = model.encode(query)\n",
    "\n",
    "hits = util.semantic_search(query_embedding, embeddings, top_k=1)\n",
    "best_hit = hits[0][0]\n",
    "\n",
    "print(\"\\n--- SUCCESSFUL RESULT ---\")\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Match: {sentences[best_hit['corpus_id']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ULPmj-zkOmE"
   },
   "source": [
    "**3. Generation (LLM Answer)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "sBotHLCVRabj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Generator Model (Flan-T5)...\n",
      "\n",
      "User Question: How do we edit genomes?\n",
      "Retrieved Context: CRISPR-Cas9 is a gene-editing tool.\n",
      "------------------------------\n",
      "AI Generated Answer: CRISPR-Cas9 is a gene-editing tool.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. Install transformers (if not already installed)\n",
    "#!pip install transformers accelerate\n",
    "\n",
    "# 2. Import libraries\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# 3. Load the the LLM\n",
    "print(\"Loading Generator Model (Flan-T5)...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "llm_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "def generate_answer(question, context):\n",
    "    # This is \"Prompt Engineering\"\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "    outputs = llm_model.generate(input_ids, max_length=50)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 4. Connect it to your previous search result\n",
    "# (We use the 'best_hit' variable from your previous code block)\n",
    "retrieved_doc = sentences[best_hit['corpus_id']]\n",
    "\n",
    "print(f\"\\nUser Question: {query}\")\n",
    "print(f\"Retrieved Context: {retrieved_doc}\")\n",
    "\n",
    "# 5. Generate the final answer\n",
    "final_answer = generate_answer(query, retrieved_doc)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"AI Generated Answer: {final_answer}\")\n",
    "print(\"-\" * 30)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

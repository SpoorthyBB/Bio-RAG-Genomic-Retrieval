{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Bio-RAG Research Assistant**"
      ],
      "metadata": {
        "id": "_CliF8pCeWyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Setup and Installation**"
      ],
      "metadata": {
        "id": "zkShUQOIUwXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "id": "qDJbxXkVYuwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# --- Step 0: Download NLTK resources (Run this once) ---\n",
        "# 'punkt' is for splitting sentences/words\n",
        "# 'stopwords' is the list of common useless words\n",
        "# 'wordnet' is the dictionary for lemmatization\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # sometimes needed for newer nltk versions\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Takes raw biological text and cleans it for the AI model.\n",
        "    Steps: Lowercase -> Tokenize -> Remove Punctuation/Stopwords -> Lemmatize\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Lowercasing\n",
        "    # Why: \"Gene\" and \"gene\" should be treated as the same thing by the model.\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Tokenization\n",
        "    # Why: Models can't read sentences; they read individual units (tokens).\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # 3. Setting up removals\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    # Customizing stopwords: In bio, we might want to keep words like 'not' (negation),\n",
        "    # but for now, we use the standard list.\n",
        "\n",
        "    # 4. Lemmatization Initialization\n",
        "    # Why: Lemmatization is smarter than Stemming.\n",
        "    # It converts \"studies\" -> \"study\" (valid word), whereas Stemming might do \"studie\" (garbage).\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    clean_tokens = []\n",
        "\n",
        "    for token in tokens:\n",
        "        # Remove punctuation and stop words\n",
        "        if token not in string.punctuation and token not in stop_words:\n",
        "            # Apply Lemmatization\n",
        "            lemma = lemmatizer.lemmatize(token)\n",
        "            clean_tokens.append(lemma)\n",
        "\n",
        "    return clean_tokens\n",
        "\n",
        "# --- Testing with a Biology Example (Relevant to Genomiki) ---\n",
        "raw_data = \"\"\"\n",
        "CRISPR-Cas9 is a unique technology that enables geneticists and medical researchers\n",
        "to edit parts of the genome by removing, adding or altering sections of the DNA sequence.\n",
        "\"\"\"\n",
        "\n",
        "processed_data = preprocess_text(raw_data)\n",
        "\n",
        "print(\"ORIGINAL TEXT:\")\n",
        "print(raw_data)\n",
        "print(\"\\n--- Processing ---\\n\")\n",
        "print(\"FINAL TOKENS (Input for the Model):\")\n",
        "print(processed_data)"
      ],
      "metadata": {
        "id": "ayMqx-NPZIBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Data Ingestion & Vector Embedding Retrieval (Semantic Search)**"
      ],
      "metadata": {
        "id": "Jktrjy8Cj4Qu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvDYgFUkNjnE"
      },
      "outputs": [],
      "source": [
        "# 1. Install the library\n",
        "!pip install sentence-transformers\n",
        "\n",
        "# 2. Import libraries\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# 3. Load the Model\n",
        "print(\"Loading Model... (Downloading from Hugging Face)\")\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# 4. Prepare Data\n",
        "sentences = [\n",
        "    \"CRISPR-Cas9 is a gene-editing tool.\",\n",
        "    \"DNA consists of two strands.\",\n",
        "    \"The mitochondria is the powerhouse of the cell.\",\n",
        "    \"Python is a programming language.\"\n",
        "]\n",
        "\n",
        "# 5. Create Embeddings\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "# 6. Search\n",
        "query = \"How do we edit genomes?\"\n",
        "query_embedding = model.encode(query)\n",
        "\n",
        "hits = util.semantic_search(query_embedding, embeddings, top_k=1)\n",
        "best_hit = hits[0][0]\n",
        "\n",
        "print(\"\\n--- SUCCESSFUL RESULT ---\")\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Match: {sentences[best_hit['corpus_id']]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Generation (LLM Answer)**"
      ],
      "metadata": {
        "id": "7ULPmj-zkOmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install transformers (if not already installed)\n",
        "!pip install transformers accelerate\n",
        "\n",
        "# 2. Import libraries\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# 3. Load the the LLM\n",
        "print(\"Loading Generator Model (Flan-T5)...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
        "llm_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
        "\n",
        "def generate_answer(question, context):\n",
        "    # This is \"Prompt Engineering\"\n",
        "    input_text = f\"question: {question} context: {context}\"\n",
        "\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "    outputs = llm_model.generate(input_ids, max_length=50)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# 4. Connect it to your previous search result\n",
        "# (We use the 'best_hit' variable from your previous code block)\n",
        "retrieved_doc = sentences[best_hit['corpus_id']]\n",
        "\n",
        "print(f\"\\nUser Question: {query}\")\n",
        "print(f\"Retrieved Context: {retrieved_doc}\")\n",
        "\n",
        "# 5. Generate the final answer\n",
        "final_answer = generate_answer(query, retrieved_doc)\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"AI Generated Answer: {final_answer}\")\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "sBotHLCVRabj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}